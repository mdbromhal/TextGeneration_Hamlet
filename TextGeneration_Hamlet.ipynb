{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Using Text Generation to Generate Hamlet-Like Text"
      ],
      "metadata": {
        "id": "7TcPWFPjlGgg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup & Imports"
      ],
      "metadata": {
        "id": "SdLG61VQQS8j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HcAkoTd-qX2s"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reading Hamlet into Python"
      ],
      "metadata": {
        "id": "RzGcgg-dq0mU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading and opening the file\n",
        "with open(\"/content/drive/MyDrive/6 Spring 2024/CSC402/Chapter16/Text_Generation/Hamlet_textfile.txt\") as f:\n",
        "  hamlet_text = f.read()"
      ],
      "metadata": {
        "id": "Y4Sd38ffrCML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Converting text to lowercase\n",
        "- Reduces the complexity of the model\n",
        "- Trains better on lowercase"
      ],
      "metadata": {
        "id": "18Bcoh1pO572"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hamlet_text = hamlet_text.lower()"
      ],
      "metadata": {
        "id": "lhDsB0xQO4OM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preview of Hamlet text\n",
        "print(hamlet_text[:80])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjwkLYmurv_b",
        "outputId": "dcf714f0-a304-41dd-ec6c-0eb0f956107b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the tragedie of hamlet\n",
            "\n",
            "actus primus. scoena prima.\n",
            "\n",
            "enter barnardo and francisc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoding the Characters\n",
        "- Creating a mapping of unique characters to integers (starting at 2)\n",
        "  - And vice versa\n",
        "- Use the mapping to encode the text before training\n",
        "- Also use to decode the generated text after prediction"
      ],
      "metadata": {
        "id": "CVMw9eJ6O_-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Shows all characters after converting to lower case\n",
        "\"\".join(sorted(set(hamlet_text.lower())))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "LEdqeOagsCwU",
        "outputId": "47166d30-fcd7-4b0a-a6d8-9759051d9387"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n !&'(),-.1:;?[]abcdefghijklmnopqrstuvwxyz\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(\"\".join(sorted(set(hamlet_text.lower())))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZiLnF7Kye0v",
        "outputId": "57e425e4-8a77-4710-961d-92ed1ccb5208"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we've seen all the characters, we want to tokenize the text\n",
        "\n",
        "**Tokenize** = convert text into vector, with each character its own number\n",
        "- We're mapping wharacters to integer sequences\n",
        "\n",
        "Text generation works better at the character level"
      ],
      "metadata": {
        "id": "F8MCq3SjwvY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_vec_layer = tf.keras.layers.TextVectorization(split='character',\n",
        "                                                   standardize='lower')\n",
        "\n",
        "text_vec_layer.adapt([hamlet_text])\n",
        "\n",
        "encoded = text_vec_layer([hamlet_text])[0]\n",
        "# Encoded[0] brings up all vectors\n",
        "# Now we have a cleaned dataset"
      ],
      "metadata": {
        "id": "txmFfCLIwtHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we want to drop the tokens 0 (pad) and 1 (unknown)\n",
        "- Pad used to make the sentences equal length\n",
        "- Unknown means the computer didn't understand the characters"
      ],
      "metadata": {
        "id": "YcDAj-o-x_FB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded -= 2"
      ],
      "metadata": {
        "id": "gUlglnCZx9VF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Br4nEN_fWuxo",
        "outputId": "d05b5a31-ee50-4119-c1ac-78e0b49909c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "162849"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And we have to subtract the 0 and 1 from the Vocabulary (number of tokens)"
      ],
      "metadata": {
        "id": "P4u82smDyXEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_tokens = text_vec_layer.vocabulary_size() - 2\n",
        "n_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fgy3HNAoyOP8",
        "outputId": "24dcef08-ab6b-4003-d04e-1d5320bf5823"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "42"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Total number of characters\n",
        "dataset_size = len(encoded)\n",
        "dataset_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WmRo22CzGp8",
        "outputId": "68ebc878-1044-4cea-8bd4-6cdaa345eb3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "162849"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- \"o be or not to be\", for example, can be the window\n",
        "- Target = sequence of character ID's representing the next window (\"to be or not to b\")\n",
        "\n",
        "\n",
        "- Input and target must be same size\n",
        "- Target = what you're predicting\n",
        "- Window = what you feed it to get the window (input)"
      ],
      "metadata": {
        "id": "aXoEq1H9z81f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "to_dataset function:\n",
        "- Takes sequence as input (encoded text)\n",
        "- Then it creates a dataset with all the windows being the desired length\n",
        "- Increases the length by 1 so we ge the next character for the target\n",
        "- Shuffles the windows (which is optional), then batches them, then splits them into input/output pairs\n",
        "- It then activates prefetching."
      ],
      "metadata": {
        "id": "NU8eQ9S6OAjX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def to_dataset(sequence, length, shuffle=False, seed=None, batch_size=32):\n",
        "  ds = tf.data.Dataset.from_tensor_slices(sequence)\n",
        "  ds = ds.window(length + 1, shift = 1, drop_remainder=True)\n",
        "  ds = ds.flat_map(lambda window_ds: window_ds.batch(length + 1))\n",
        "\n",
        "  if shuffle:\n",
        "    ds = ds.shuffle(100_000, seed=seed)\n",
        "  ds = ds.batch(batch_size)\n",
        "  return ds.map(lambda window: (window[:, :-1], window[:, 1:])).prefetch(1)"
      ],
      "metadata": {
        "id": "7HNp5sXiQ1nf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating the Training Sequences\n",
        "- Converting the text to sequences of characters\n",
        "- Each input sequence is a fixed length of characters (for example, 100)\n",
        "- The corresponding target sequence is the same sequence shifted by 1 character"
      ],
      "metadata": {
        "id": "2a-FbvCNPOix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "length = 100\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "train_set = to_dataset(encoded[:1_000_000], length=length, shuffle=True, seed=42)\n",
        "# Set aside 90% of text for training\n",
        "\n",
        "valid_set = to_dataset(encoded[1_000_000:1_060_000], length=length)\n",
        "# Use 5% for validation\n",
        "\n",
        "test_set = to_dataset(encoded[1_060_000:], length=length)\n",
        "# And use last 5% for testing"
      ],
      "metadata": {
        "id": "zW44tkZDRks0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building the Char-RNN Model\n",
        "- Defining a Sequential model in Keras with LSTM layers\n",
        "- Dense layer with a softmax activation function\n",
        "  - Use for output layer & must have n_tokens units\n",
        "    - Want to produce a probability for each possible character at each time step\n",
        "    - The n_tokens output probabilities should sum up to 1 at each time step (probability of each character)\n",
        "    - Then we use the softmax activation function (chooses max; character with highest probability)\n",
        "  - Output layer has as many neurons as there are unique characters in the text\n",
        "- Embedding layer is the first layer, which encodes the character ID's\n",
        "  - Number of input dimensions = number of distinct character ID's\n",
        "    - 2D tensors of shape [batch size, window length]\n",
        "  - Number output dimensions = hyperparameter we can tune (set to 16 for now)\n",
        "    - 3D tensor shape [batch size, window length, embedding size]\n",
        "- Then we compile model with 'sparse_categorical_crossentropy' loss and Nadam optimizer\n",
        "- The last layer is the prediction layer\n",
        "  - In deep learning, last layer's # neurons = # possible predictions\n",
        "- Middle layers are hidden\n",
        "  - If we get bad accuracy, add more layers\n",
        "- First layer = Embedding Layer\n",
        "  - Window batches fed into the model"
      ],
      "metadata": {
        "id": "KGWbCFESPdD_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **HOW DO I USE LSTM WITH THIS MODEL?**"
      ],
      "metadata": {
        "id": "ic1HsLuZ-ZsU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the Model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=n_tokens, output_dim=16),\n",
        "    tf.keras.layers.GRU(128, return_sequences=True),\n",
        "    tf.keras.layers.Dense(n_tokens, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "VuiqGQm7RETx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss function\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='nadam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Checkpoint = save best only\n",
        "model_ckpt = tf.keras.callbacks.ModelCheckpoint(\n",
        "    'hamlet_model', monitor='val_accuracy', save_best_only=True)"
      ],
      "metadata": {
        "id": "SEYUuzv8RKW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_set, validation_data=valid_set, epochs=5,\n",
        "                    callbacks=[model_ckpt])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXC58h43SZAx",
        "outputId": "bd9f2adf-b62a-486a-fffc-5225c1b0c29a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "   5086/Unknown - 652s 123ms/step - loss: 1.6596 - accuracy: 0.5020"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r5086/5086 [==============================] - 653s 123ms/step - loss: 1.6596 - accuracy: 0.5020\n",
            "Epoch 2/5\n",
            "5086/5086 [==============================] - ETA: 0s - loss: 1.2521 - accuracy: 0.6132"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r5086/5086 [==============================] - 636s 120ms/step - loss: 1.2521 - accuracy: 0.6132\n",
            "Epoch 3/5\n",
            "5086/5086 [==============================] - ETA: 0s - loss: 1.1670 - accuracy: 0.6392"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r5086/5086 [==============================] - 637s 120ms/step - loss: 1.1670 - accuracy: 0.6392\n",
            "Epoch 4/5\n",
            "5086/5086 [==============================] - ETA: 0s - loss: 1.1380 - accuracy: 0.6480"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r5086/5086 [==============================] - 663s 126ms/step - loss: 1.1380 - accuracy: 0.6480\n",
            "Epoch 5/5\n",
            "5086/5086 [==============================] - ETA: 0s - loss: 1.1236 - accuracy: 0.6525"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r5086/5086 [==============================] - 624s 118ms/step - loss: 1.1236 - accuracy: 0.6525\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- loss = 1.1157\n",
        "- accuracy = 0.6563"
      ],
      "metadata": {
        "id": "-eORQJDPoiPa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The above model doesn't handle text preprocessing\n",
        "- The below model does\n",
        "  - Wrapped in a final model with tf.keras.layers.TextVectorization layer as first layer\n",
        "  - tf.keras.layers.Lambda layer to subtract 2 from the character ID's\n",
        "    - Since we're not using the padding and unknown tokens"
      ],
      "metadata": {
        "id": "8VnKqoZLRNp6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hamlet_model = tf.keras.Sequential([\n",
        "    text_vec_layer,\n",
        "    tf.keras.layers.Lambda(lambda X: X - 2), # No pad or unknown characters\n",
        "    model\n",
        "])"
      ],
      "metadata": {
        "id": "TiJ-WNlsTddJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the Model\n",
        "- Compiling the model with a suitable loss function and optimizer\n",
        "- Uses categorical crossentropy as the loss function since this is a multi-class classification problem\n",
        "- Trains the model on the sequences previously prepared\n",
        "- Uses model checkpoints and early stopping to prevent overfitting"
      ],
      "metadata": {
        "id": "EWYZeir1Po3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_proba = hamlet_model.predict(['tis but our Fantasie, And will not let beleef'])[0, -1] # Treason, Treason\n",
        "\n",
        "y_pred = tf.argmax(y_proba)\n",
        "\n",
        "text_vec_layer.get_vocabulary()[y_pred + 2]"
      ],
      "metadata": {
        "id": "maNmyTaaTwCS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "c57e8bdc-f1b1-42c2-9e3d-dd34a38bbd05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 630ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'e'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating the Text w. Char-RNN Model\n",
        "- The function below generates text with the trained model\n",
        "  - This function accepts a seed string and the number of characters to generate\n",
        "  - Outputs a new text that mimics the style of the book\n",
        "- **Greedy Decoding** = when we generate a letter, add it to the end of the text, then generate the next letter of that text, and so on\n",
        "  - Often leads to repetative guesses\n",
        "- So instead, we sample the next character *randomly*\n",
        "  - Probability = estimated probability (tf.random.categorical() function)\n",
        "    - This function samples random class indices, given the class log probabilities (logits)\n",
        "  - Generate more diverse and interesting text"
      ],
      "metadata": {
        "id": "soLlLOqdP4Uc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "log_probas = tf.math.log([[0.5, 0.4, 0.1]]) #Probas = 50%, 40%, 10%\n",
        "tf.random.set_seed(42)\n",
        "tf.random.categorical(log_probas, num_samples=8) # Draw 8 samples"
      ],
      "metadata": {
        "id": "R0WQkLv0k0lf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d749f651-1afd-4aa2-ffc6-703b006c9fc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 8), dtype=int64, numpy=array([[0, 1, 0, 2, 1, 0, 0, 1]])>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Below we're translating the character id from the character's vector to the character that we humans can read.\n",
        "\n",
        "- To have more control on diversity of generated text, divide logits by temperature\n",
        "  - [0 = high-probability characters, 1 = equal-probability characters]\n",
        "  - Lower = preferred with precise text\n",
        "  - Higher = with more creative and diverse text\n",
        "\n",
        "- next_char() function = custom helper function\n",
        "  - Uses temperature approach to pick the next character to add to the input text"
      ],
      "metadata": {
        "id": "7LjnLKzildkX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def next_char(text, temperature=1):\n",
        "  y_proba = hamlet_model.predict([text])[0, -1:]\n",
        "  rescaled_logits = tf.math.log(y_proba) / temperature\n",
        "  char_id = tf.random.categorical(rescaled_logits, num_samples=1)[0, 0]\n",
        "  return text_vec_layer.get_vocabulary()[char_id + 2]"
      ],
      "metadata": {
        "id": "vixBTae0lGDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- extend_text() function = another helper function\n",
        "  - Gets the next character and appends it to the given text"
      ],
      "metadata": {
        "id": "Ts6J0kQrPK0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extend_text(text, n_chars=50, temperature=1):\n",
        "  for _ in range(n_chars):\n",
        "    text += next_char(text, temperature)\n",
        "  return text"
      ],
      "metadata": {
        "id": "uBgbbXQilmkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(extend_text('tis but our Fantasie, And will not let beleefe ', temperature=0.01))"
      ],
      "metadata": {
        "id": "uzUvvP8clw1U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98b78c16-8976-469a-da4e-a7993c850e2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 207ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 93ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "tis but our Fantasie, And will not let beleefe time of his land of reason you thinke it was a pai\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "My favorite output so far: \"tis but our Fantasie, And will not let beleefe the world and father lord, i haue seene the world\""
      ],
      "metadata": {
        "id": "hs_ZPip4TO9X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment and Analyze\n",
        "- Experiments with the different hyperparameters, such as the sequence length, number of LSTM units, and training duration\n",
        "- Analyzes how these changes affect the quality and coherence of the generated text"
      ],
      "metadata": {
        "id": "-IfVZ9_xQERH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(extend_text('tis but our Fantasie, And will not let beleefe ', temperature=1))"
      ],
      "metadata": {
        "id": "MLsks-G5l3f_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "623dff0e-ad0b-4cf3-9fd2-3302fdc96b95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "tis but our Fantasie, And will not let beleefe car'st they friends you conuert intaine condere-wi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(extend_text('tis but our Fantasie, And will not let beleefe ', temperature=100))"
      ],
      "metadata": {
        "id": "13dLz2pyl9G2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "784e5fef-44c6-4e6e-ee3a-2eb20830a65c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "tis but our Fantasie, And will not let beleefe  fyt;1o[.'hm,zjlwnzlse-bws[est(zvptsd)f-c(,(ew!l&&\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, the higher the temperature, the more gibberish the model spits out."
      ],
      "metadata": {
        "id": "Y2ScSQ3Jz8s8"
      }
    }
  ]
}